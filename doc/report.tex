\documentclass{article}
\usepackage{booktabs}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage[nottoc]{tocbibind}
\usepackage{hyperref} % Include this package to use URLs
\usepackage{url}
\hypersetup{%
    pdfborder = {0 0 0}
}
% \usepackage[]{biblatex}
% \usepackage[nottoc]{tocbibind}
\fancyhf{} % Clear header/footer
\renewcommand{\headrulewidth}{0pt} % Remove header rule
\usepackage[english]{babel}
%Includes "References" in the table of contents
% \addbibresource{bib}
% \AtBeginBibliography{\small}
\usepackage{appendix}

\begin{document}

\title{Bibsearch Engine Report}
\author{Galanopoulou Rafaila 7115112200033\\
Kalopisis Ioannis 7115112200011\\
 Notaris  Ioannis 7115112200038}
\date{\today}


\maketitle
\newpage
\tableofcontents

\newpage

\section*{Resources}
Code can be found in this repo: \url{https://github.com/rafailagln/BibSearchEngine}

The actual application will be up and running \underline{for the next two weeks}
here: \url{http://46.101.232.219:8080/bibengine/}

\section{Idea}

We have embarked on the ambitious task of developing a bibliographic search engine that 
harnesses the vast amount of data available in the Crossref dataset. With a staggering size 
of 1.7 million documents out of 
157 GB compressed (1 TB uncompressed), this dataset encompasses publication metadata from 
approximately 134 million publications, including full citation data for 60 million of them. 
Our objective was to create a tool that enables users to efficiently explore this extensive 
collection of scholarly resources, and to build a system that can be scaled to handle even 
larger datasets in the future.

To handle the scale and complexity of this dataset, we designed our search engine as a 
distributed system, leveraging the power of eight nodes. One node serves as the front-end, 
handling user queries and managing the user interface, while the remaining seven nodes function 
as the back-end, responsible for data storage and retrieval. In order to optimize query 
performance, we implemented a MongoDB database to store the index containing only the 
essential information of each zipped json file such as title, 
abstract, and URL in memory, ensuring rapid access and retrieval. Eventually, 
MongoDB is used to store metadata of each entry. 
% containing metadata of the index 

In addition to establishing a robust infrastructure, we implemented a clear ranking 
system to enhance the relevance and accuracy of search results.
Our ranking algorithm considers was built based on BM25F, %TODO add citation
a widely established ranking algorithm based on the TF-IDF model. This algorithm takes into
account the input query and match as many words from title and abstract. 
Furthermore, an indexing mechanism has been developed
to facilitate quick search operations across the vast dataset, enabling users to obtain relevant 
results within seconds.

To further enhance the user experience and provide valuable suggestions, we integrated a machine 
learning model into our search engine. This model analyzes user queries and generates 
alternative query suggestions, assisting users in refining their search terms and discovering 
additional relevant publications. By leveraging the power of machine learning, we aim to 
continuously improve the search engine's capability to adapt and cater to users' needs.

%\section{Architecture}
% probably add a diagram
\section{Implementation}
\subsection{Ranking Implementation}

The ranking has been implemented using the BM25F algorithm, which is a variant of the BM25 
ranking algorithm that is designed to handle documents with multiple fields. Each field 
contributes to the final score of a document. The following steps outline how the ranking 
process works:

\begin{enumerate}
    \item The \texttt{BM25F} class is defined, taking the inverted index and the total number of 
    documents in the collection as input.
    \item The \texttt{bm25f} method in the \texttt{BM25F} class calculates the BM25F score for a 
    list of documents and a query. It calculates the score for each field (e.g., title, 
    abstract) and then sums up the scores to obtain the final score for each document.
    \item The \texttt{\_idf\_calculation} method calculates the IDF (Inverse Document Frequency) 
    for each query term. IDF is a measure of how important a term is in the collection.
    \item The \texttt{\_tf\_field\_calculation} method calculates the term frequency (TF) for 
    each query term and field. TF is a measure of how frequently a term appears in a document's 
    field.
    \item The \texttt{\_algorith\_parameters} method returns the algorithm parameters (k1 and b) 
    for each field. These parameters control the impact of term frequency and field length in 
    the scoring process.
    \item The \texttt{SearchEngine} class is defined, taking the inverted index and the maximum 
    number of results to return as input.
    \item The \texttt{search} method in the \texttt{SearchEngine} class performs the search 
    operation using the BM25F ranking algorithm. It cleans the query, counts the results, 
    performs boolean search if needed, ranks the documents using BM25F, and calculates the final 
    score for each document based on a combination of the BM25F score and the referenced\_by 
    score.
    \item The \texttt{BooleanInformationRetrieval} class is defined, which performs boolean 
    search by finding the documents that contain all the words in the query. The documents are 
    then ranked by the number of words they contain.
%    \item The \texttt{Relevancy} class provides a cosine similarity method to calculate the 
%    similarity between two sentences.
    \item The \texttt{sort\_documents} method in the \texttt{SearchEngine} class sorts the 
    documents based on their scores in descending order.
\end{enumerate}

The mathematical formula of the BM25F algorithm is:

\begin{multline}
    \text{{score}}(Q, D) = \sum_{i=1}^{n} \text{{IDF}}(q_i) \cdot \\
    \left[ \frac{\sum_{j=1}^{m} W_{f_j} \cdot \text{{TF}}(q_i, F_{f_j}, D) \cdot (k1_{f_j} + 1)}{\sum_{j=1}^{m} W_{f_j} \cdot \text{{TF}}(q_i, F_{f_j}, D) + k1_{f_j} \cdot \left(1 - b_{f_j} + b_{f_j} \cdot \frac{\text{{length}}(F_{f_j}, D)}{\text{{avg\_length}}(F_{f_j})}\right)} \right]
\end{multline}

Where:
\begin{itemize}
  \item $Q = \{q_1, q_2, ..., q_n\}$ is a query with $n$ terms.
  \item $D$ is a document.
  \item $IDF(q_i)$ is the inverse document frequency of term $q_i$.
  \item $TF(q_i, F_{f_j}, D)$ is the term frequency of term $q_i$ in field $f_j$ of document $D$.
  \item $W_{f_j}$ is the weight of field $f_j$.
  \item $k1_{f_j}$ and $b_{f_j}$ are free parameters, specific to each field $f_j$.
  \item $\text{{length}}(F_{f_j}, D)$ is the length of the field $f_j$ in the document $D$.
  \item $\text{{avg\_length}}(F_{f_j})$ is the average length of field $f_j$ in the collection.
\end{itemize}

This formula calculates the BM25F score for a given document $D$ and query $Q$ by summing 
the contributions of each query term $q_i$, where the contribution is determined by the IDF of 
the term and the weighted term frequency in each field $f_j$. The term frequency is normalized
by the field length and average field length in the collection. The weight of each field $f_j$ is
given by $W_{f_j}$. The parameters $k1_{f_j}$ and $b_{f_j}$ are specific to each field, and they control the 
scaling by term frequency and field length, respectively \cite{Rost2022}. 

In summary, the ranking process involves calculating the IDF, TF, and BM25F \cite{PJAGIF10} 
scores for each 
document and query term, and then combining these scores to obtain the final ranking. The 
process takes into account factors such as term frequency, field length, and document references 
to determine the relevance and ranking of documents.
% TODO add citation and mathematical formula

\subsubsection{Boolean Information Retrieval Algorithm}
The Boolean Information Retrieval algorithm was used to initially narrow down the results. 
If the number of texts containing any of the words found in the query, then we use this 
algorithm to give an initial score to the documents. For each word of the query that contains 
the text, it gets a point, and then a classification is made based on this point. In the end, 
the texts with the highest score are the ones that pass to the next stage of the BM25F algorithm.

\subsubsection{Preprocessor Package}

The preprocessor package provides functionality to clean and preprocess the data before 
performing any analysis or indexing. The package consists of the \texttt{DataCleaner} class, 
which performs various data cleaning operations. Here's a summary of the implemented 
functionality:

\begin{itemize}
    \item \texttt{clean\_data(data)}: Cleans the given data by performing several data cleaning 
    operations. It removes HTML tags, converts the data to lowercase, tokenizes the data into a 
    list of words, and removes stopwords and punctuation from the data. The cleaned data is 
    returned as a list of words.
    \item \texttt{\_\_html\_strip(data)}: Removes HTML tags from the given data.
    \item \texttt{\_\_lower\_string(data)}: Converts the given data to lowercase.
    \item \texttt{\_\_tokenizer(data)}: Tokenizes the given data into a list of words.
    \item \texttt{\_\_punctuation\_deletion(data)}: Removes stopwords and punctuation from the 
    given data.
\end{itemize}

The \texttt{DataCleaner} class utilizes the NLTK library for various operations. It downloads 
the necessary NLTK resources, such as stopwords and tokenizers, during initialization.

The data cleaning operations performed by the \texttt{DataCleaner} class help in preparing the 
data for further analysis or indexing. It removes unnecessary HTML tags, converts the data to 
lowercase, tokenizes the data into words, and removes common stopwords and punctuation. The 
cleaned data can then be used for various natural language processing tasks.

Overall, the preprocessor package provides essential functionality for cleaning and 
preprocessing data, ensuring that it is in a suitable format for further analysis or indexing.

\subsection{Indexer Package}

The indexer package provides functionality to create and manage indexes for a MongoDB database. 
It consists of the \texttt{Reader} and \texttt{IndexCreator} classes, along with supporting 
classes \texttt{Metadata} and \texttt{InfoClass}.

%\subsubsection{Reader}
%
%The \texttt{Reader} class handles reading operations from a MongoDB database. It provides 
%methods to retrieve a collection, read all documents from a collection, and read a limited 
%number of documents from a collection. Here are the key methods of the \texttt{Reader} class:
%
%\begin{itemize}
%    \item \texttt{\_\_init\_\_(connection\_string)}: Initializes a \texttt{Reader} object with 
%    the given MongoDB connection string.
%    \item \texttt{get\_collection(db\_name, collection\_name)}: Retrieves a collection from the 
%    MongoDB database.
%    \item \texttt{read\_collection(collection)}: Reads all documents from the provided 
%    collection.
%    \item \texttt{read\_limited\_collection(collection, limit)}: Reads a limited number of 
%    documents from the provided collection.
%\end{itemize}

\subsubsection{Metadata}

The \texttt{Metadata} class handles metadata operations related to the index. It maintains 
information about document's field lengths, average lengths, and references. Here are the key methods of 
the \texttt{Metadata} class:

\begin{itemize}
    \item \texttt{\_\_init\_\_(db\_name, metadata\_collection)}: 
    Initializes the \texttt{Metadata} object with the given MongoDB database name and collection name.
%    \item \texttt{update\_doc\_num()}: Updates the total number of documents.
    \item \texttt{add\_doc\_length\_field(doc\_id, length, field)}: Adds the length of a 
    specific field in a document.
    \item \texttt{increase\_average\_length(length, field)}: Increases the average length of a 
    specific field.
    \item \texttt{calculate\_average\_length()}: Calculates the average length for each field.
    \item \texttt{add\_referenced\_by(doc\_id, referenced)}: Adds the number of times a document 
    is referenced by other documents.
    \item \texttt{normalize\_referenced\_by()}: Normalizes the referenced\_by values.
    \item \texttt{set\_total\_docs(num)}: Sets the total number of documents.
    \item \texttt{load()}: Loads the metadata from the MongoDB collection.
    \item \texttt{save()}: Saves the metadata to the MongoDB collection.
\end{itemize}

\subsubsection{InfoClass}

The \texttt{InfoClass} class represents information about a specific document in the index. It 
stores the type, position, and document ID associated with the information. The class provides a 
method to convert the instance into a dictionary representation.

\subsubsection{IndexCreator}

The \texttt{IndexCreator} class handles the creation and loading of the index. It utilizes the 
\texttt{TrieIndex} class from the \texttt{distributed} module for indexing. Here are the key 
methods of the \texttt{IndexCreator} class:

\begin{itemize}
    \item \texttt{\_\_init\_\_(db, metadata\_collection, db\_name='M151', \\
    index\_collection='Index')}: Initializes an instance of \texttt{IndexCreator} with the given 
    parameters.
    \item \texttt{create\_load\_index()}: Creates or loads the index. It creates the index if 
    the collection is empty or loads the existing index.
    \item \texttt{node\_adder(\_id, cleaned\_words, \_type)}: Adds nodes to the index.
\end{itemize}

The \texttt{IndexCreator} class utilizes the \texttt{DataCleaner} class from the preprocessor 
package for cleaning the data before indexing. It also interacts with the MongoDB database using 
the \texttt{db.connection} module.

Overall, the indexer package provides functionality to create, load, and manage indexes for a 
MongoDB database. It incorporates data cleaning, metadata management, and indexing operations, 
ensuring efficient retrieval and analysis of the indexed data.


\subsection{APIRequester Package}

The APIRequester package provides functionality for sending HTTP requests to the API Server 
(see section \ref{sec:apiserver}). It includes the \texttt{APIRequester} class, which handles 
authentication and sending POST requests to the 
API endpoints.


The \texttt{APIRequester} class is responsible for creating and sending HTTP requests to the 
API. When a node forwards a request and gets a bad response it will use this class to communicate 
with the API Server via the \texttt{update\_config} endpoint (see section~\ref{sec:endpoints}). 
It requires the base URL, username, and password for authentication. Here are the key methods 
of the \texttt{APIRequester} class:

\begin{itemize}
    \item \texttt{\_\_init\_\_(base\_url, username, password)}: Initializes an 
    \texttt{APIRequester} object with the base URL, username, and password.
    \item \texttt{post\_update\_config\_endpoint(data)}: Sends a POST request to the 
    '\texttt{up-\\date\_config}' endpoint of the API with the provided configuration data. Returns the 
    parsed JSON response if the request is successful, otherwise raises an exception with the 
    corresponding status code.
\end{itemize}

The \texttt{APIRequester} class utilizes the \texttt{http.client} module to establish a 
connection with the API and send HTTP requests. It also utilizes the \texttt{base64} and 
\texttt{json} modules for authentication and parsing JSON responses, respectively. 
The \texttt{urllib.parse} module is used to parse the base URL.

Overall, the APIRequester package provides a convenient way to authenticate and send POST 
requests to an API.  

\subsection{API Server}
\label{sec:apiserver}

The API server is implemented using the FastAPI framework. It provides several endpoints for 
handling different types of requests. To understand why these endpoints where created we will 
briefly analyse the end-to-end workflow. When a user submits a query in the front end, a request
is made to the \texttt{search\_ids} endpoint and returns the full list of document ids for that 
query. Subsequently, it will make another request to the \texttt{fetch\_data} endpoint to retrieve 
information for the 10 highest ranked documents. A new request to \texttt{fetch\_data} endpoint 
is made every time the user changes the page. 


The server supports basic authentication and uses a 
configuration manager and request wrapper to handle configuration updates and API requests, 
respectively.

\subsubsection{API Server Setup}

The server is set up using the FastAPI framework. It initializes the FastAPI application, sets 
up logging, and configures CORS middleware to handle cross-origin requests. The configuration 
file paths are defined, and instances of the ConfigManager, RequestWrapper, and IniConfig 
classes are created.

\subsubsection{Authentication}

The server supports basic authentication using the provided credentials. 
The \texttt{get\_current\_username} function validates the provided username and password 
against the configured credentials. If the credentials are incorrect, an HTTPException with 
status code 401 (Unauthorized) is raised. In the current implementation, only the 
\texttt{update\_config} endpoint is locked.

\subsubsection{Endpoints}
\label{sec:endpoints}

The API server provides the following endpoints:

\begin{itemize}
    \item \texttt{GET /search\_ids/\{query}\}: Retrieves a list of document IDs matching the 
    provided query.
    \item \texttt{GET /alternate\_queries/\{query\}}: Retrieves a list of alternate queries 
    based on the provided query.
    \item \texttt{POST /fetch\_data/}: Fetches data for the specified document IDs.
    \item \texttt{POST /update\_config}: Updates the configuration and saves it.
\end{itemize}

The \texttt{search\_ids} endpoint calls the \texttt{search\_ids} method of the 
\texttt{RequestWrapper} class asynchronously, using a ThreadPoolExecutor to handle the request 
concurrently. The result is returned as a list of document IDs.

The \texttt{alternate\_queries} endpoint returns a static list of alternate queries.

The \texttt{fetch\_data} endpoint calls the \texttt{fetch\_data} method of the 
\texttt{RequestWrapper} class asynchronously, using a ThreadPoolExecutor to handle the request 
concurrently. The result is returned as a list of dictionaries containing the fetched data for 
each document.

The \texttt{update\_config} endpoint reads the updated configuration data from the HTTP request, 
saves it using the \texttt{ConfigManager}, and updates the \texttt{neighbour\\\_nodes} property of 
the \texttt{RequestWrapper}.

\subsubsection{Running the Server}

The server is run using the \texttt{uvicorn} package, which serves the FastAPI application. The 
host and port are read from the configuration file.

Overall, the API server provides a RESTful API with authentication and support for concurrent 
requests. It allows users to search for document IDs, fetch data, and update the configuration 
of the system.

\subsection{\texttt{Distributed} Package}

The \texttt{distributed} package offers functionality for handling JSON documents stored in a 
distributed manner across multiple files and servers. It includes the following key features:

\subsubsection{Shard Calculation}
The package provides a function \texttt{get\_shard(doc\_id, num\_shards)} to determine the shard 
ID based on the document ID and the total number of shards. All ids within a node have 
the same remainder when divided with the node count. This way, every node knows exactly where to 
ask when it needs to get data from another node.

\subsubsection{FastJsonLoader Class}
FastJsonLoader class facilitates loading and accessing JSON documents stored in a distributed manner,
 specifically through sharding across multiple nodes. Documents are stored in a specified directory 
 as compressed files in .gz format. Upon initialization, the class decompresses each document, 
 selectively extracts the required values, and then recompresses the file before storing it into 
 memory. MongoDB is used to save metadata such as document IDs, file associations, private index etc. 
 It provides methods to load documents into memory, retrieve specific documents based on their IDs or 
 retrieve all documents. The class is designed to handle changes in file contents, and if such changes 
 are detected, it logs an informational message and skips the changed file. 

\begin{itemize}
    \item \texttt{\_\_init\_\_(...)}: The class constructor allows for setting parameters such 
    as the folder path containing the JSON files, the number of documents per file, the node ID, 
    the total node count, and the MongoDB collection for storing document IDs.
    
    \item \texttt{load\_ids()}: Loads the document IDs from MongoDB.
    
    \item \texttt{save\_ids()}: Saves the document IDs to MongoDB.
    
    \item \texttt{load\_documents()}: Reads JSON files from the specified folder and organizes 
    the documents into compressed data files based on the desired number of documents per file. 
    It also handles metadata about the document files.
    
    \item \texttt{get\_data(doc\_ids, sort\_by\_doc\_id=False)}: Retrieves data for the given 
    document IDs. It can also sort the results based on the order of the provided doc\_ids.
    
    \item \texttt{insert\_documents(new\_documents)}: Inserts new documents into the JSON files.
    
    \item \texttt{delete\_documents(doc\_ids\_to\_delete)}: Deletes documents from the JSON 
    files.
    
    \item \texttt{count\_documents\_in\_folder()}: Counts the total number of documents present 
    in the specified folder.
    
    \item \texttt{get\_all\_documents()}: Retrieves all documents from the JSON files.
    
    \item \texttt{id\_file\_exists()}: Checks if the document IDs exist in MongoDB.
\end{itemize}

This package provides a convenient and efficient way to manage and work with distributed JSON 
documents across multiple files.

\subsection{Implementation of a Distributed Bibliographic Search Engine}

In this section, we will explore the deployment of a bibliographic search engine that has 
been implemented in a distributed manner. The search engine is designed to handle large 
volumes of bibliographic data and provide efficient search capabilities. The system consists 
of multiple nodes that work together to distribute the workload and improve performance. Here, 
we will discuss the implementation details of the \texttt{DistributedNode} class, which 
represents an individual node in the distributed system.

\subsubsection{DistributedNode Class}
\label{sec:distributed}

The \texttt{DistributedNode} class is responsible for managing the operations of a single node 
in the distributed bibliographic search engine as well a communicating with all the other nodes. 
It handles incoming requests, performs document loading and indexing, and facilitates distributed 
search operations. 

On startup, all nodes receive a heartbeat from the leader for confirmation, followed by 
instructions to load documents and create the index. This leader's role is needed only 
during initiation. The system can handle node failures, with minor data loss whike the 
server is down in the absence of replication. When a node sends a request and gets a bad 
response it will update the config of the API server to stop forwarding urls to the 
corresponding node. When the node recovers, it updates the others to reestablish its 
presence on the network.

Let's go through the key components and methods of this class to understand its functionality
 in more detail.


\subsubsection*{Initialization}

The \texttt{\_\_init\_\_} method initializes a \texttt{DistributedNode} object with various 
parameters. These parameters include the node's ID, host address, port, file paths, database 
information, SSL encryption details, API credentials, and document indexing metadata.


\subsubsection*{Handling Requests}

The \texttt{handle\_request} method processes incoming requests and generates appropriate 
responses. It receives a JSON-encoded request, extracts the action type, and performs the 
corresponding operation. The supported actions include heartbeat, document loading, index 
creation/loading, data retrieval, updating node status, ID-based search, setting the starting 
document ID, inserting new documents, retrieving the configuration, and setting the leader.

\subsubsection*{Running the Node}

The \texttt{run} method starts the server and listens for incoming connections on the specified 
host and port. It creates a new thread for each accepted connection to handle the request 
asynchronously. The server continuously runs in a loop, accepting and processing requests.


\subsubsection*{Heartbeat and Initialization}

The \texttt{check\_heartbeats} method is used for checking the heartbeats of the neighboring 
nodes and initiating the document loading process. It sends heartbeat requests to all nodes and 
verifies their status. If the current node is the leader and it is the first boot, it starts a 
separate thread to continuously check the heartbeats. On subsequent boots, the method retrieves 
the configuration from other nodes and starts the document loading and index creation process.

\subsubsection*{Data Retrieval and Search}

The \texttt{handle\_get\_data} method handles the retrieval of data based on IDs. It splits the 
IDs based on the node count and distributes the requests to the respective nodes. It uses 
multithreading to perform data retrieval in parallel and aggregates the results.

The \texttt{search\_ids} method performs a search operation based on a query string. Similar to 
data retrieval, it distributes the search requests across nodes using multithreading and 
combines the results.


\subsubsection*{Configuration Management}


The deployment of a distributed bibliographic search engine requires the coordination and 
collaboration of multiple nodes. The \texttt{DistributedNode} class plays a crucial role in 
managing the operations of an individual node within the system. It handles requests, 
facilitates distributed document loading and indexing, and supports efficient data retrieval and 
search capabilities. By leveraging the power of a distributed architecture, the search engine 
can efficiently process large volumes of bibliographic data and provide accurate search results.

\section{Suggest Alternative Queries}

The inclusion of this task within our project aimed to delve into the myriad possibilities 
for generating alternative text, given its widespread utilization in real-world applications. 
After conducting an extensive literature review on the subject, we proceeded to experiment 
with various techniques. Enclosed herein are general insights pertaining to the diverse 
methodologies we employed.

\subsection{KNN}

The K-Nearest Neighbors (KNN)~\cite{ZQXT20} algorithm is a simple, easy-to-implement 
supervised machine learning algorithm that can be used to solve both classification and 
regression problems. In the context of Natural Language Processing (NLP), you might use it to 
predict synonyms based on word vectorization.

To predict synonyms using K-Nearest Neighbors (KNN), we first convert words into vectors 
using an algorithm like Word2Vec, GloVe, or FastText, which arrange semantically similar 
words close together in a high-dimensional space. We then train a KNN model on these vectors, 
such that each word (represented by a vector) has 'neighbors' based on distance in this 
vector space. When predicting synonyms for a given word, we input the vector of that word 
into the KNN model and find the 'K' nearest vectors (words). The words associated with these 
vectors are then considered the predicted synonyms for the given word.

Bear in mind that this is a simplified overview and actual results can vary significantly 
based on the choice of vectorization method~\cite{ZQXT20}, the value of 'K', and the specific 
dataset used.
It's also worth noting that KNN might not be the most suitable or efficient model for this 
task, especially with large vocabularies, as it doesn't scale well with the size of the 
dataset. More advanced methods, such as using neural networks, have shown better performance 
in tasks involving semantics.

\subsection{Word2Vec}
In 2012, Thomas Mikolov, an intern at Microsoft, found a way to encode the meaning On words 
in a modest number of vector dimensions.CMikolor trained a neural networks to predict word 
occurrences near each target word. In 2013, once at Google, Mikolov and his teammates 
released the software for creating these word vectors and called it Word2vec~\cite{MCCD13}.

Word2vec learns the meaning of words merely by processing a large corpus of unlabeled text. 
No one has to label the words in the Word2vec vocabulary. No one has to tell the Word2vec 
algorithm that Marie Curie is a scientist, that the Timbers are a soccer team, that Seattle 
is a city, or that Portland is a city in both Oregon and Maine. And no one has to tell 
Word2vec that soccer is a sport, or that a team is a group of people, or that cities are both 
places as well as communities. Word2vec can learn that and much more, all on its own! All you 
need is a corpus large enough to mention Marie Curie and Timbers and Portland near other 
words associated with science or soccer or cities.

\paragraph{Word vectors} are numerical vector representations of word semantics, or meaning, including literal and implied meaning. So word vectors can capture the connotation of words, like “peopleness,” “animalness,” “placeness,” “thingness,” and even “conceptness.” And they combine all that into a dense vector (no zeros) of floating point values. This dense vector enables queries and logical reasoning

All words in your corpus will be represented by numerical vectors, similar to the word-topic vectors discussed in chapter 4. Only this time the topics mean something more specific, more precise. In LSA, words only had to occur in the same document to have their meaning “rub off” on each other and get incorporated into their word-topic vectors. For Word2vec word vectors, the words must occur near each other—typically fewer than five words apart and within the same sentence. And Word2vec word vector topic weights can be added and subtracted to create new word vectors that mean something!

Word2vec allows you to transform your natural language vectors of token occurrence counts and frequencies into the vector space of much lower-dimensional Word2vec vectors. In this lower-dimensional space, you can do your math and then convert back to a natural language space. You can imagine how useful this capability is to a chatbot, search engine, question answering system, or information extraction algorithm.

\subsubsection*{How to compute word2vec representations}

Word2Vec uses two main algorithms to create these word embeddings: Continuous Bag of Words (CBOW) and Skip-gram.

\paragraph{CBOW} This algorithm takes the context of each word as the 
input and tries to predict the word corresponding to the context. The 
context is defined as a window of surrounding words. For example, if 
we consider the sentence "The cat sat on the mat", and our target 
word is 'sat', with a window size of 2, the context words are 'The', 
'cat', 'on', 'the'. The CBOW model tries to predict the target word 
'sat' given these context words.

\paragraph{Skip-gram} The Skip-gram model works in the reverse manner 
of the CBOW model. It tries to predict the context words given a 
target word. Using the same sentence "The cat sat on the mat", and 
the target word 'sat', the Skip-gram model will try to predict the 
context words 'The', 'cat', 'on', 'the' given the target word 'sat'.

The mathematical formulations of these models involve the use of 
softmax function and negative sampling. The softmax function is used 
to turn scores into probabilities, which are used to predict words. 
Negative sampling is a simplification used to make the computation 
faster by updating only a small percentage of the model's weights.

The softmax function is often used as the activation function in the output layer of neural 
networks when the network’s goal is to learn classification problems. The softmax will squash 
the output results between 0 and 1, and the sum of all outputs will always add up to 1. That 
way, the results of an output layer with a softmax function can be considered as 
probabilities.

For each of the K output nodes, the softmax output value can be calculated using the normalized exponential function:

\[
\text{{softmax}}(z_i) = \frac{{e^{z_i}}}{{\sum_{j=1}^{K} e^{z_j}}}
\]
where \( z_i \) represents the input value to the \( i \)th output node, \( e \) is the base of the natural logarithm (approximately 2.71828), and \( \sum_{j=1}^{K} e^{z_j} \) denotes the sum of the exponential values of all input nodes.

The softmax function is often used as the activation function in the output layer of neural networks when the network’s goal is to learn classification problems. The softmax will squash the output results between 0 and 1, and the sum of all outputs will always add up to 1. That way, the results of an output layer with a softmax function can be considered as probabilities.


\subsection{Other approaches}

\subsubsection{Sent2Vec} This is an extension of the Word2Vec model to 
generate embeddings for entire sentences or short texts. It considers 
the order of the words in the text, which can provide additional 
context and meaning~\cite{MZ20}. Sent2Vec provides a simple and efficient way to 
represent sentences in vector space which can be beneficial for tasks 
like text classification, clustering, and similarity assessment.

\subsubsection{BERT (Bidirectional Encoder Representations from 
Transformers)} 
BERT~\cite{DCLT19} is a more advanced model for generating word 
embeddings. Unlike Word2Vec, which only uses local context to 
understand a word, BERT uses both the left and the right context, 
which means it considers the entire sentence. BERT is pre-trained on 
a large corpus of text and then fine-tuned for specific tasks, such 
as question answering or sentiment analysis. BERT's bidirectional 
approach allows it to understand the context and the semantic meaning 
of a word better, leading to more accurate results in many NLP tasks.

\subsubsection{T5 (Text-to-Text Transfer Transformer)}
T5 is a model proposed by Google in 2019, designed to 
improve various language tasks by treating every task as a "text-to-text" problem. That 
means, it takes text as input and returns text as output. T5 is pretrained on a large corpus 
of text from the internet and can be fine-tuned on specific tasks such as translation, 
summarization, sentiment analysis, and more.

\subsubsection{Transformers}
Transformers are a type of model architecture introduced in a paper called 
"Attention is All You Need"~\cite{ASNG17}. The key innovation is the self-attention mechanism 
that allows the model to weigh the importance of words in an input sequence when generating 
an output sequence. This makes them very effective for many NLP tasks, especially those 
involving understanding the context of words in sentences.

\subsubsection*{Limitations}

While these models have significantly advanced the field of NLP, they do come with a few limitations:
\begin{itemize}
    \item Resource-Intensive: These models are typically large and require substantial computational resources to train. This makes them inaccessible to individuals or organizations with limited resources.

    \item Opaque: Like many deep learning models, these models are often seen as "black boxes" because it can be difficult to understand why they make specific decisions or predictions.

    \item Biases: These models can learn and perpetuate the biases in the data they were trained on. This includes gender, racial, or ideological biases present in the text.

    \item Contextual understanding: While these models are exceptional at encoding syntactic relationships within text, they can struggle with tasks that require a higher level of understanding, such as reasoning or nuanced comprehension of the text.

    \item Training on New Tasks: They often require large amounts of data to be fine-tuned for new tasks. However, obtaining these large annotated datasets can be challenging.

    \item Sensitive Information: They can potentially generate or expose sensitive information, as they are trained on large-scale internet text data. They might output inappropriate content or reveal information they were trained on.
\end{itemize}

\section{Deployment}
For the deployment of our bibliographic search engine, the following approach has been followed.
The front-end of our system has been developed using the Spring Boot framework.
The back-end functionalities have been implemented in Python, aiming for efficient data storage, 
retrieval, and processing. 

To ensure the quality and reliability of our codebase, we have implemented automated testing 
procedures. Using the GitHub Actions of the repository, 
continuous integration and deployment pipelines have been set up that run a series of unit tests 
on each commit. This ensures that any issues or bugs are identified early on, allowing for 
prompt resolution and maintaining a stable system.

The distributed database approach was adopted, utilizing a total of eight nodes. 
This distributed setup allowed us to efficiently handle the immense size and complexity of the 
Crossref dataset. In the appendix~\ref{apx:hardware-specs}, please find detailed hardware 
specifications for each node, including information such as CPU, memory, and storage capacity. 
By strategically distributing the workload across multiple nodes, we aimed to achieve enhanced 
performance, improved fault tolerance, and the ability to handle a high volume of user queries 
simultaneously. 

\section{Observations}

\subsection{Indexer}
In order to be able to search the documents we store and serve the queries that come, we chose 
to use the structure of the inverted index. An inverted index is a data structure used to create 
a full-text search index, enabling fast, keyword-based searches. It is one of the most critical 
components of search engines.

The term $inverted$ comes from the way the index is built: instead of listing, for each document 
the terms it contains, it lists for each term, the documents that contain it, thus "inverting" 
the relationship between documents and terms. Inverted indexes are used because they allow very 
fast keyword-based searches. Once the inverted index is built, finding all documents containing 
a particular keyword is as simple as looking up the keyword in the index, which is a very fast 
operation. Without an inverted index, a keyword-based search would require scanning through all 
documents in the collection, which is impractical for large document collections.

To reduce the search time for a single word, the inverted index must be in main memory, so that 
we do not have to access the disk every time. In order to reduce the size of the index and fit it 
entirely in memory, we built it using a $Trie$ data structure. So we observed a dramatic reduction 
in the size of the inverted index. Also a side benefit observed using this structure was an 
increase in search speed.

\subsection{BM25F Parallelization}
To increase the performance of the BM25F algorithm we tried to parallelize the calculation of the 
scores of the documents. We studied two basic techniques, to parallelize computation at process 
level and at thread level. In both cases the number of documents of each node of the system, was 
divided into processes or threads, the scores of each chunk were calculated in parallel, and at 
the end the results were collected in a common structure that contained all the texts and their 
scores. But through experiments, both with the number of documents, but also with the number of 
processes or threads, we did not see an improvement in the performance. Below we explain the 
possible reasons why we did not see a performance improvement for each technique:

\begin{itemize}
    \item {\texttt{Process} Level Parallelization}: Multiprocessing in Python has an overhead 
    associated with it due to the need to spin up new processes, communicate between them 
    (which involves pickling and unpickling data), and then collect the results. This overhead 
    can be substantial, especially for smaller tasks or faster computations where the time 
    spent setting up and managing the processes can outweigh the benefits of parallel execution.
    BM25F algorithm is not be sufficiently complex or time-consuming to benefit from parallel 
    execution, particularly given the overheads associated with multiprocessing. The overheads 
    may include:

    \begin{itemize}
        \item The time taken to start and stop processes.
        \item The time taken to divide the task into chunks.
        \item The time taken to communicate data between processes (serialization and deserialization).
    \end{itemize}

    \item {\texttt{Thread} Level Parallelization}: Python's GIL, or Global Interpreter Lock, 
    is a mutex that prevents parallelization of the BM25F algorithm at the thread level. The 
    GIL allows only one thread to execute at a time in a single process to ensure thread 
    safety. While the GIL ensures fast and efficient execution for single-threaded programs,
     it significantly hampers the performance of multi-threaded CPU-bound programs. Despite 
     having multiple threads within a process, the GIL restricts only one thread from 
     executing at a time, rendering the advantages of multi-core processors ineffective. 
     Consequently, the GIL essentially results in single-threaded execution, even in multi-
     threaded Python programs.
\end{itemize}

We can easily observe that the BM25F algorithm is a CPU intensive task. Since we are not able 
to run it with multiple threads we won't see any performance improvement. It would be useful 
to run the algorithm at the process level, but we observe that this is also an expensive task 
and reduces performance. If we used different CPU architecture and a bigger volume of data 
we could try the multi-process technique to speedup performance.

\subsection{Alternative query suggestion}

We selected the Word2Vec model, using phrases and entire sentences as the corpus, owing to our 
limited resources~. This approach facilitates the generation of alternative sentences by 
randomly selecting similar words. However, we acknowledge the potential for improvements in 
this method. One proposal involves opting for the "best" similar words instead of a random 
selection, where "best" is defined by the highest cosine similarity metric provided by 
Word2Vec's \texttt{most\_similar} function. However, this approach has a potential downside of 
generating identical sentences when more than one sentence is requested due to the lack of 
variation. \footnote{The whole process of trying different models can be found in the 
notebook \url{https://github.com/rafailagln/BibSearchEngine/blob/main/src/back/model/
model_train.ipynb}}



To tackle this issue, we could introduce a degree of randomness via a weighted choice method. 
This would increase the likelihood of picking the most similar words while still providing 
some variation in the results. However, this approach is primarily based on word-level 
similarity and may not always result in meaningful or grammatically correct sentences. If the 
'best' is intended to indicate the preservation of the original meaning, fluency, or 
grammaticality of sentences, more complex approaches might be required. These could 
potentially involve other natural language processing techniques or resources to ensure the 
coherency and semantic alignment of the generated alternative sentences.

\newpage
\bibliographystyle{unsrt}
\bibliography{bib}

\newpage
\appendix
\section{Hardware Specifications}
\label{apx:hardware-specs}
\begin{table}[H]
    \centering
    \caption{RAM Information}
    \begin{tabular}{ll}
      \toprule
      \textbf{Property} & \textbf{Value} \\
      \midrule
      \multicolumn{2}{l}{\textbf{Firmware}} \\
      Vendor & DigitalOcean \\
      Version & 20171212 \\
      Date & 12/12/2017 \\
      Size & 96KiB \\
      Capabilities & virtualmachine \\
      \midrule
      \multicolumn{2}{l}{\textbf{Memory}} \\
      Physical ID & 1000 \\
      Size & 8GiB \\
      Capabilities & ecc \\
      Configuration & errordetection=multi-bit-ecc \\
      \midrule
      \multicolumn{2}{l}{\textbf{Memory Device}} \\
      Array Handle & 0x1000 \\
      Size & 8 GB \\
      Form Factor & DIMM \\
      Set & None \\
      Locator & DIMM 0 \\
      Type & RAM \\
      Manufacturer & QEMU \\
      \bottomrule
    \end{tabular}
  \end{table}

  \newpage
  \pagestyle{fancy} % Apply the modified page style to the current page
  \begin{table}[H]
    \centering
    \caption{System Information}
    \begin{tabular}{ll}
      \toprule
      \textbf{Property} & \textbf{Value} \\
      \midrule
      \multicolumn{2}{l}{\textbf{Computer}} \\
      Product & Droplet \\
      Vendor & DigitalOcean \\
      Version & 20171212 \\
      Width & 64 bits \\
      Capabilities & smbios-2.8 dmi-2.8 smp vsyscall32 \\
      Configuration & boot=normal family=DigitalOcean\_Droplet \\
      \midrule
      \multicolumn{2}{l}{\textbf{Motherboard}} \\
      Product & Droplet \\
      Vendor & DigitalOcean \\
      Version & 20171212 \\
      \midrule
      \multicolumn{2}{l}{\textbf{Firmware}} \\
      Description & BIOS \\
      Vendor & DigitalOcean \\
      Physical ID & 0 \\
      Version & 20171212 \\
      Date & 12/12/2017 \\
      Size & 96KiB \\
      Capabilities & virtualmachine \\
      \midrule
      \multicolumn{2}{l}{\textbf{CPU}} \\
      Description & CPU \\
      Product & DO-Regular \\
      Vendor & Intel Corp. \\
      Physical ID & 400 \\
      Bus Info & cpu@0 \\
      Version & 6.63.2 \\
      Slot & CPU 0 \\
      Size & 2GHz \\
      Capacity & 2GHz \\
      Width & 64 bits \\
      Configuration & cores=4 enabledcores=4 microcode=1 threads=1 \\
      \midrule
      \multicolumn{2}{l}{\textbf{Memory}} \\
      Description & System Memory \\
      Physical ID & 1000 \\
      Size & 8GiB \\
      Capabilities & ecc \\
      Configuration & errordetection=multi-bit-ecc \\
      \midrule
      \multicolumn{2}{l}{\textbf{Memory Device}} \\
      Description & DIMM RAM \\
      Vendor & QEMU \\
      Physical ID & 0 \\
      Slot & DIMM 0 \\
      Size & 8GiB \\
      \midrule
      \multicolumn{2}{l}{\textbf{Host Bridge}} \\
      Product & 440FX - 82441FX PMC [Natoma] \\
      Vendor & Intel Corporation \\
      Physical ID & 100 \\
      Width & 32 bits \\
      Clock & 33MHz \\
      \midrule
      \bottomrule
    \end{tabular}
  \end{table}
  \newpage
  
  \begin{table}[H]
    \centering
    \caption{CPU Information}
    \begin{tabular}{l l}
      \toprule
      \textbf{Property} & \textbf{Value} \\
      \midrule
      \textbf{Architecture} & x86\_64 \\
      CPU op-mode(s) & 32-bit, 64-bit \\
      Address sizes & 40 bits physical, 48 bits virtual \\
      Byte Order & Little Endian \\
      \midrule
      \textbf{CPU(s)} & 4 \\
      On-line CPU(s) list & 0-3 \\
      \textbf{Vendor ID} & GenuineIntel \\
      Model name & DO-Regular \\
      CPU family & 6 \\
      Model & 63 \\
      Thread(s) per core & 1 \\
      Core(s) per socket & 4 \\
      Socket(s) & 1 \\
      Stepping & 2 \\
      BogoMIPS & 3990.62 \\
      \midrule
      \textbf{Virtualization features} & \\
      Virtualization & VT-x \\
      Hypervisor vendor & KVM \\
      Virtualization type & full \\
      \midrule
      \textbf{Caches (sum of all)} & \\
      L1d & 128 KiB (4 instances) \\
      L1i & 128 KiB (4 instances) \\
      L2 & 16 MiB (4 instances) \\
      \midrule
      \textbf{NUMA} & \\
      NUMA node(s) & 1 \\
      NUMA node0 CPU(s) & 0-3 \\
      
      \bottomrule
    \end{tabular}
  \end{table}

\end{document}



